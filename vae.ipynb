{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp vae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vae\n",
    "\n",
    "> Memories that can generalize, and imagine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 16;\n",
       "                var nbb_unformatted_code = \"# hide\\nfrom nbdev.showdoc import *\\n\\n%load_ext nb_black\\n%matplotlib inline\\n%config InlineBackend.figure_format='retina'\\n%config IPCompleter.greedy=True\";\n",
       "                var nbb_formatted_code = \"# hide\\nfrom nbdev.showdoc import *\\n\\n%load_ext nb_black\\n%matplotlib inline\\n%config InlineBackend.figure_format='retina'\\n%config IPCompleter.greedy=True\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *\n",
    "\n",
    "%load_ext nb_black\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 31;\n",
       "                var nbb_unformatted_code = \"# export\\nfrom torchvision.datasets import FashionMNIST\\nimport numpy as np\\n\\nimport torch\\nimport torch as th\\nimport torch.optim as optim\\nimport torch.nn.functional as F\\nfrom torch import nn\\n\\nfrom torch.utils.tensorboard import SummaryWriter\";\n",
       "                var nbb_formatted_code = \"# export\\nfrom torchvision.datasets import FashionMNIST\\nimport numpy as np\\n\\nimport torch\\nimport torch as th\\nimport torch.optim as optim\\nimport torch.nn.functional as F\\nfrom torch import nn\\n\\nfrom torch.utils.tensorboard import SummaryWriter\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "from torchvision.datasets import FashionMNIST\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch as th\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 38;\n",
       "                var nbb_unformatted_code = \"# export\\nclass VAE(nn.Module):\\n    def __init__(self, input_dim=784, latent_dim=20):\\n        super(VAE, self).__init__()\\n        # Set dims\\n        self.input_dim = int(input_dim)\\n        self.latent_dim = int(latent_dim)\\n        # Init the layers in the deep net\\n        self.fc1 = nn.Linear(self.input_dim, 400)\\n        self.fc21 = nn.Linear(400, self.latent_dim)\\n        self.fc22 = nn.Linear(400, self.latent_dim)\\n        self.fc3 = nn.Linear(self.latent_dim, 400)\\n        self.fc4 = nn.Linear(400, self.input_dim)\\n\\n    def encode(self, x):\\n        h1 = F.relu(self.fc1(x))\\n        return self.fc21(h1), self.fc22(h1)\\n\\n    def _reparameterize(self, mu, logvar):\\n        std = torch.exp(0.5 * logvar)\\n        eps = torch.randn_like(std)\\n        return mu + eps * std\\n\\n    def decode(self, z):\\n        \\\"\\\"\\\"Expand a latent memory\\\"\\\"\\\"\\n        h3 = F.relu(self.fc3(z))\\n        return torch.sigmoid(self.fc4(h3))\\n\\n    def sample(self, n, device=None):\\n        \\\"\\\"\\\"Use noise to sample latent space.\\\"\\\"\\\"\\n        with torch.no_grad():\\n            x = torch.randn(n, self.latent_dim)\\n            x = x.to(device)\\n            samples = self.decode(x)\\n            return samples\\n\\n    def forward(self, x):\\n        \\\"\\\"\\\"Get a reconstructed image\\\"\\\"\\\"\\n        mu, logvar = self.encode(x.view(-1, self.input_dim))\\n        z = self._reparameterize(mu, logvar)\\n        return self.decode(z), mu, logvar\";\n",
       "                var nbb_formatted_code = \"# export\\nclass VAE(nn.Module):\\n    def __init__(self, input_dim=784, latent_dim=20):\\n        super(VAE, self).__init__()\\n        # Set dims\\n        self.input_dim = int(input_dim)\\n        self.latent_dim = int(latent_dim)\\n        # Init the layers in the deep net\\n        self.fc1 = nn.Linear(self.input_dim, 400)\\n        self.fc21 = nn.Linear(400, self.latent_dim)\\n        self.fc22 = nn.Linear(400, self.latent_dim)\\n        self.fc3 = nn.Linear(self.latent_dim, 400)\\n        self.fc4 = nn.Linear(400, self.input_dim)\\n\\n    def encode(self, x):\\n        h1 = F.relu(self.fc1(x))\\n        return self.fc21(h1), self.fc22(h1)\\n\\n    def _reparameterize(self, mu, logvar):\\n        std = torch.exp(0.5 * logvar)\\n        eps = torch.randn_like(std)\\n        return mu + eps * std\\n\\n    def decode(self, z):\\n        \\\"\\\"\\\"Expand a latent memory\\\"\\\"\\\"\\n        h3 = F.relu(self.fc3(z))\\n        return torch.sigmoid(self.fc4(h3))\\n\\n    def sample(self, n, device=None):\\n        \\\"\\\"\\\"Use noise to sample latent space.\\\"\\\"\\\"\\n        with torch.no_grad():\\n            x = torch.randn(n, self.latent_dim)\\n            x = x.to(device)\\n            samples = self.decode(x)\\n            return samples\\n\\n    def forward(self, x):\\n        \\\"\\\"\\\"Get a reconstructed image\\\"\\\"\\\"\\n        mu, logvar = self.encode(x.view(-1, self.input_dim))\\n        z = self._reparameterize(mu, logvar)\\n        return self.decode(z), mu, logvar\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim=784, latent_dim=20):\n",
    "        super(VAE, self).__init__()\n",
    "        # Set dims\n",
    "        self.input_dim = int(input_dim)\n",
    "        self.latent_dim = int(latent_dim)\n",
    "        # Init the layers in the deep net\n",
    "        self.fc1 = nn.Linear(self.input_dim, 400)\n",
    "        self.fc21 = nn.Linear(400, self.latent_dim)\n",
    "        self.fc22 = nn.Linear(400, self.latent_dim)\n",
    "        self.fc3 = nn.Linear(self.latent_dim, 400)\n",
    "        self.fc4 = nn.Linear(400, self.input_dim)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def _reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        \"\"\"Expand a latent memory\"\"\"\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def sample(self, n, device=None):\n",
    "        \"\"\"Use noise to sample latent space.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            x = torch.randn(n, self.latent_dim)\n",
    "            x = x.to(device)\n",
    "            samples = self.decode(x)\n",
    "            return samples\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Get a reconstructed image\"\"\"\n",
    "        mu, logvar = self.encode(x.view(-1, self.input_dim))\n",
    "        z = self._reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 33;\n",
       "                var nbb_unformatted_code = \"# export\\ndef loss_function(recon_x, x, mu, logvar, input_dim):\\n    \\\"\\\"\\\"Reconstruction + KL divergence losses summed over all elements and batch\\\"\\\"\\\"\\n    BCE = F.binary_cross_entropy(recon_x, x.view(-1, input_dim), reduction=\\\"sum\\\")\\n\\n    # see Appendix B from VAE paper:\\n    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\\n    # https://arxiv.org/abs/1312.6114\\n    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\\n    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\\n\\n    return BCE + KLD\";\n",
       "                var nbb_formatted_code = \"# export\\ndef loss_function(recon_x, x, mu, logvar, input_dim):\\n    \\\"\\\"\\\"Reconstruction + KL divergence losses summed over all elements and batch\\\"\\\"\\\"\\n    BCE = F.binary_cross_entropy(recon_x, x.view(-1, input_dim), reduction=\\\"sum\\\")\\n\\n    # see Appendix B from VAE paper:\\n    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\\n    # https://arxiv.org/abs/1312.6114\\n    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\\n    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\\n\\n    return BCE + KLD\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "def loss_function(recon_x, x, mu, logvar, input_dim):\n",
    "    \"\"\"Reconstruction + KL divergence losses summed over all elements and batch\"\"\"\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, input_dim), reduction=\"sum\")\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 37;\n",
       "                var nbb_unformatted_code = \"# export\\ndef train(train_batch, model, optimizer, device, input_dim):\\n    model.train()\\n    batch = train_batch.to(device)\\n    optimizer.zero_grad()\\n    recon_batch, mu, logvar = model(train_batch)\\n    loss = loss_function(recon_batch, train_batch, mu, logvar, input_dim)\\n    loss.backward()\\n    optimizer.step()\\n    return loss\";\n",
       "                var nbb_formatted_code = \"# export\\ndef train(train_batch, model, optimizer, device, input_dim):\\n    model.train()\\n    batch = train_batch.to(device)\\n    optimizer.zero_grad()\\n    recon_batch, mu, logvar = model(train_batch)\\n    loss = loss_function(recon_batch, train_batch, mu, logvar, input_dim)\\n    loss.backward()\\n    optimizer.step()\\n    return loss\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "def train(train_batch, model, optimizer, device, input_dim):\n",
    "    model.train()\n",
    "    batch = train_batch.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    recon_batch, mu, logvar = model(train_batch)\n",
    "    loss = loss_function(recon_batch, train_batch, mu, logvar, input_dim)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 35;\n",
       "                var nbb_unformatted_code = \"# export\\ndef test(test_data, model, device):\\n    model.eval()\\n    test_loss = 0\\n    with torch.no_grad():\\n        for i, (data, _) in enumerate(test_data):\\n            data = data.to(device)\\n            recon_batch, mu, logvar = model(data)\\n            test_loss += loss_function(recon_batch, data, mu, logvar).item()\\n\\n    return test_loss\";\n",
       "                var nbb_formatted_code = \"# export\\ndef test(test_data, model, device):\\n    model.eval()\\n    test_loss = 0\\n    with torch.no_grad():\\n        for i, (data, _) in enumerate(test_data):\\n            data = data.to(device)\\n            recon_batch, mu, logvar = model(data)\\n            test_loss += loss_function(recon_batch, data, mu, logvar).item()\\n\\n    return test_loss\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "def test(test_data, model, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_data):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
